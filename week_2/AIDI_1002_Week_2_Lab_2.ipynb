{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDI 1002 Lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In question one of this lab we are performing text pre-processing for a sample data downloaded from Kaggle. Second question is dedicated to analyzing of time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ahmad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ahmad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ahmad/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# this is for tokenization\n",
    "nltk.download('punkt')\n",
    "# this packages are for lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv file\n",
    "df = pd.read_csv('sample_kaggle_data.csv')\n",
    "# setting up lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of functions that are responsible for preprocessing\n",
    "def load_csv(path: str) -> 'dataframe':\n",
    "    '''\n",
    "    This functions loads csv file\n",
    "    input: path to the file\n",
    "    output: pandas dataframe\n",
    "    '''\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def drop_columns(df: 'dataframe', cols: list) -> 'dataframe':\n",
    "    '''\n",
    "    This funciton remove the columns that are specified\n",
    "    input: cols -> list of columns that will be dropped\n",
    "    output: datafrae\n",
    "    '''\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "\n",
    "# NOTE: This function should only be used if its applicable to your\n",
    "# project. It removes numbers from the text\n",
    "def remove_numbers(df: 'dataframe', col: str) -> 'dataframe':\n",
    "    '''\n",
    "    This function remove numbers from the specified column in\n",
    "    the dataframe\n",
    "    input: df -> dataframe, col: targeted column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[col] = df[col].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# NOTE: This function should only be used if its applicable to your\n",
    "# project. It removes punctuations from the text\n",
    "def remove_punctuation(df: 'dataframe', col:str) -> 'dataframe':\n",
    "    '''\n",
    "    This function removes punctuations from the specified colum\n",
    "    input: df -> dataframe, col: column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "def remove_white_space(df: 'dataframe', col:str) -> 'dataframe':\n",
    "    '''\n",
    "    This function removes leading and ending white spaces from a text\n",
    "    input: df -> dataframe, col: column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "def tokenize(col: str) -> list:\n",
    "    '''\n",
    "    This function tokenizes the specified column\n",
    "    input: col -> column\n",
    "    output: list of tokenized sentence\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(col)\n",
    "    return tokens\n",
    "    \n",
    "def create_token_col(df: 'dataframe', col: str) -> 'dataframe':\n",
    "    '''\n",
    "    This function creates a column of tokenized column\n",
    "    input: df -> dataframe, col -> column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[f'{col}_tokenized'] = df[col].apply(tokenize)\n",
    "\n",
    "def lem(col):\n",
    "    return [lemmatizer.lemmatize(word) for word in col]\n",
    "\n",
    "def create_lem_col(df:'dataframe', col: str) -> 'dataframe':\n",
    "    '''\n",
    "    This function lemmatize the column\n",
    "    input: col -> column\n",
    "    output: datafreame\n",
    "    '''\n",
    "    df[f'{col}_lemmatized'] = df[col].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = load_csv('sample_kaggle_data.csv')\n",
    "drop_columns(df1, ['id','label'])\n",
    "remove_punctuation(df1, 'tweet')\n",
    "remove_numbers(df1, 'tweet')\n",
    "remove_white_space(df1, 'tweet')\n",
    "create_token_col(df1, 'tweet')\n",
    "create_lem_col(df1, 'tweet_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixes to be implemented:\n",
    "Lemmatizer doesn't work properly, use POS-tag so that lemmatizer knows what kind of word its dealing wiht"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aidi1002",
   "language": "python",
   "name": "aidi1002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
