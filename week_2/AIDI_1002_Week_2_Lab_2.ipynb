{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIDI 1002 Lab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In question one of this lab we are performing text pre-processing for a sample data downloaded from Kaggle. Second question is dedicated to analyzing of time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "# this is for tokenization\n",
    "# nltk.download('punkt')\n",
    "# # this packages are for lemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Vairable\n",
    "# Stop word\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "# Lemmatizer\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "# English Words\n",
    "ENGLISH_WORDS = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation\n",
       "5   6      0  [2/2] huge fan fare and big talking before the...\n",
       "6   7      0   @user camping tomorrow @user @user @user @use...\n",
       "7   8      0  the next school year is the year for exams.ð...\n",
       "8   9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9  10      0   @user @user welcome here !  i'm   it's so #gr..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the csv file\n",
    "df = pd.read_csv('sample_kaggle_data.csv')\n",
    "print(len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has almost 32k rows. The texts are from twitter and as its visable alot of stop words, punctuations and non englihs words or characters. We are assuming that in our application those words are not needed. We created set of functions to remove numbers, remove punctuations, remove starting and ending spaces, tokenize columns, remove stop words, lemmatize wrods according to the tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of functions that are responsible for preprocessing\n",
    "def load_csv(path: str) -> 'dataframe':\n",
    "    '''\n",
    "    This functions loads csv file\n",
    "    input: path to the file\n",
    "    output: pandas dataframe\n",
    "    '''\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def drop_columns(df: 'dataframe', cols: list):\n",
    "    '''\n",
    "    This funciton remove the columns that are specified\n",
    "    input: cols -> list of columns that will be dropped\n",
    "    output: datafrae\n",
    "    '''\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "\n",
    "def lower_case(df: 'dataframe', col: str) -> 'dataframe':\n",
    "    '''\n",
    "    This function drop null value columns and\n",
    "    change the column to lower case\n",
    "    input: df -> dataframe, col -> column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df = df[df[col].notna()]\n",
    "    df[col] = df[col].str.lower()\n",
    "    return df\n",
    "    \n",
    "# NOTE: This function should only be used if its applicable to your\n",
    "# project. It removes numbers from the text\n",
    "def remove_numbers(df: 'dataframe', col: str):\n",
    "    '''\n",
    "    This function remove numbers from the specified column in\n",
    "    the dataframe\n",
    "    input: df -> dataframe, col: targeted column\n",
    "    '''\n",
    "    df[col] = df[col].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "    \n",
    "# NOTE: This function should only be used if its applicable to your\n",
    "# project. It removes punctuations from the text\n",
    "def remove_punctuation(df: 'dataframe', col:str) -> 'dataframe':\n",
    "    '''\n",
    "    This function removes punctuations from the specified colum\n",
    "    input: df -> dataframe, col: column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    \n",
    "def remove_white_space(df: 'dataframe', col:str):\n",
    "    '''\n",
    "    This function removes leading and ending white spaces from a text\n",
    "    input: df -> dataframe, col: column\n",
    "    output: dataframe\n",
    "    '''\n",
    "    df[col] = df[col].str.strip()\n",
    "        \n",
    "\n",
    "def tokenize(col: str) -> list:\n",
    "    '''\n",
    "    This function tokenizes the specified column\n",
    "    input: col -> column\n",
    "    output: list of tokenized sentence\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(col)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_token_col(df: 'dataframe', col: str):\n",
    "    '''\n",
    "    This function creates a column of tokenized column\n",
    "    input: df -> dataframe, col -> column\n",
    "    '''\n",
    "    df[f'{col}_tokenized'] = df[col].apply(tokenize)\n",
    "\n",
    "    \n",
    "def remove_non_english(arr: list) -> list:\n",
    "    '''\n",
    "    This function removes non-english words from the column\n",
    "    input: arr -> list of words\n",
    "    output: list of words excluding non-english words\n",
    "    '''\n",
    "    just_english = []\n",
    "    for word in arr:\n",
    "        if word in ENGLISH_WORDS:\n",
    "            just_english.append(word)\n",
    "    return just_english\n",
    "\n",
    "\n",
    "def apply_non_english(df: 'dataframe', col: str):\n",
    "    '''\n",
    "    This function applies remove non english function above\n",
    "    to the specified column\n",
    "    input: df -> dataframe, col -> column\n",
    "    '''\n",
    "    df[col] = df[col].apply(remove_non_english)\n",
    "\n",
    "    \n",
    "def remove_stop_words(arr: list) -> list:\n",
    "    '''This function removes english stop words\n",
    "    input: arr -> array that we wont to remove stop\n",
    "    words from\n",
    "    output: an array without stop words\n",
    "    '''\n",
    "    no_stop_word = []\n",
    "    for word in arr:\n",
    "        if word not in STOP_WORDS:\n",
    "            no_stop_word.append(word)\n",
    "    return no_stop_word\n",
    "\n",
    "def apply_stop_words(df: 'dataframe', col: str):\n",
    "    '''\n",
    "    This function removes the stopwords in the column\n",
    "    input: col -> column name\n",
    "    output\n",
    "    '''\n",
    "    df[col] = df[col].apply(remove_stop_words)\n",
    "\n",
    "    \n",
    "def word_type(word: str) -> str:\n",
    "    '''\n",
    "    This function returns type of word noun, verb ...\n",
    "    input: word -> word\n",
    "    output: string indicating type of word\n",
    "    '''\n",
    "    return wordnet.synsets(word)[0].pos()\n",
    "\n",
    "\n",
    "def lem(col):\n",
    "    lemmatized_words = []\n",
    "    for word in col:\n",
    "        try:\n",
    "            lemmatized = LEMMATIZER.lemmatize(word,word_type(word))\n",
    "            lemmatized_words.append(lemmatized)\n",
    "        except:\n",
    "            lemmatized = LEMMATIZER.lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def create_lem_col(df:'dataframe', col: str) -> 'dataframe':\n",
    "    '''\n",
    "    This function lemmatize the column\n",
    "    input: col -> column\n",
    "    output: datafreame\n",
    "    '''\n",
    "    df[f'{col}_lemmatized'] = df[col].apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_one():\n",
    "    '''This function applies higher level pipeline\n",
    "    functions\n",
    "    '''\n",
    "    print('loading dataframe...')\n",
    "    df1 = load_csv('sample_kaggle_data.csv')\n",
    "    print('removing redundant columns...')\n",
    "    drop_columns(df1, ['id','label'])\n",
    "    print('changing words to lower case...')\n",
    "    df1 = lower_case(df1, 'tweet')\n",
    "    print('removing punctuations...')\n",
    "    remove_punctuation(df1, 'tweet')\n",
    "    print('removing numbers...')\n",
    "    remove_numbers(df1, 'tweet')\n",
    "    print('removing white spaces...')\n",
    "    remove_white_space(df1, 'tweet')\n",
    "    print('tokenizing targetted column...')\n",
    "    create_token_col(df1, 'tweet')\n",
    "    print('removing non-english words...')\n",
    "    apply_non_english(df1,'tweet_tokenized')\n",
    "    print('remove stopwords...')\n",
    "    apply_stop_words(df1, 'tweet_tokenized')\n",
    "    print('lemmatize...')\n",
    "    create_lem_col(df1, 'tweet_tokenized')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataframe...\n",
      "removing redundant columns...\n",
      "changing words to lower case...\n",
      "removing punctuations...\n",
      "removing numbers...\n",
      "removing white spaces...\n",
      "tokenizing targetted column...\n",
      "removing non-english words...\n",
      "remove stopwords...\n",
      "lemmatize...\n"
     ]
    }
   ],
   "source": [
    "df1 = pipeline_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>tweet_tokenized_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31952</th>\n",
       "      <td>user you went too far with user</td>\n",
       "      <td>[user, went, far, user]</td>\n",
       "      <td>[user, go, far, user]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31953</th>\n",
       "      <td>good morning instagram shower water berlin ber...</td>\n",
       "      <td>[good, morning, shower, water, berlin, girl]</td>\n",
       "      <td>[good, morning, shower, water, berlin, girl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31954</th>\n",
       "      <td>holiday   bull up you will dominate your bull ...</td>\n",
       "      <td>[holiday, bull, dominate, bull, direct, whatev...</td>\n",
       "      <td>[holiday, bull, dominate, bull, direct, whatev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31955</th>\n",
       "      <td>less than  weeks ð",
       "ðð¼ð¹ððµ user ibizabringito...</td>\n",
       "      <td>[less, user]</td>\n",
       "      <td>[le, user]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31956</th>\n",
       "      <td>off fishing tomorrow user carnt wait first tim...</td>\n",
       "      <td>[fishing, tomorrow, user, wait, first, time]</td>\n",
       "      <td>[fishing, tomorrow, user, wait, first, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>ate user isz that youuuðððððððððâï</td>\n",
       "      <td>[ate, user]</td>\n",
       "      <td>[ate, user]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>to see nina turner on the airwaves trying to w...</td>\n",
       "      <td>[see, turner, trying, wrap, mantle, genuine, h...</td>\n",
       "      <td>[see, turner, try, wrap, mantle, genuine, hero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>[listening, sad, morning, work, sad]</td>\n",
       "      <td>[listening, sad, morning, work, sad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>user sikh temple vandalised in in calgary wso ...</td>\n",
       "      <td>[user, temple, act]</td>\n",
       "      <td>[user, temple, act]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>thank you user for you follow</td>\n",
       "      <td>[thank, user, follow]</td>\n",
       "      <td>[thank, user, follow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "31952                    user you went too far with user   \n",
       "31953  good morning instagram shower water berlin ber...   \n",
       "31954  holiday   bull up you will dominate your bull ...   \n",
       "31955  less than  weeks ð\n",
       "ðð¼ð¹ððµ user ibizabringito...   \n",
       "31956  off fishing tomorrow user carnt wait first tim...   \n",
       "31957                 ate user isz that youuuðððððððððâï   \n",
       "31958  to see nina turner on the airwaves trying to w...   \n",
       "31959  listening to sad songs on a monday morning otw...   \n",
       "31960  user sikh temple vandalised in in calgary wso ...   \n",
       "31961                      thank you user for you follow   \n",
       "\n",
       "                                         tweet_tokenized  \\\n",
       "31952                            [user, went, far, user]   \n",
       "31953       [good, morning, shower, water, berlin, girl]   \n",
       "31954  [holiday, bull, dominate, bull, direct, whatev...   \n",
       "31955                                       [less, user]   \n",
       "31956       [fishing, tomorrow, user, wait, first, time]   \n",
       "31957                                        [ate, user]   \n",
       "31958  [see, turner, trying, wrap, mantle, genuine, h...   \n",
       "31959               [listening, sad, morning, work, sad]   \n",
       "31960                                [user, temple, act]   \n",
       "31961                              [thank, user, follow]   \n",
       "\n",
       "                              tweet_tokenized_lemmatized  \n",
       "31952                              [user, go, far, user]  \n",
       "31953       [good, morning, shower, water, berlin, girl]  \n",
       "31954  [holiday, bull, dominate, bull, direct, whatev...  \n",
       "31955                                         [le, user]  \n",
       "31956       [fishing, tomorrow, user, wait, first, time]  \n",
       "31957                                        [ate, user]  \n",
       "31958  [see, turner, try, wrap, mantle, genuine, hero...  \n",
       "31959               [listening, sad, morning, work, sad]  \n",
       "31960                                [user, temple, act]  \n",
       "31961                              [thank, user, follow]  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.tail(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aidi1002",
   "language": "python",
   "name": "aidi1002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
